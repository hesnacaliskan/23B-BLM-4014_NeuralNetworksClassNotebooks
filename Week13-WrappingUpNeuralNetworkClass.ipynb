{"cells":[{"cell_type":"code","source":"!pip install torchtext==0.10.0","metadata":{"cell_id":"10006ac6e16d4794a744a754b0260572","source_hash":"60f55b34","execution_start":1685535200927,"execution_millis":28256,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Collecting torchtext==0.10.0\n  Downloading torchtext-0.10.0-cp39-cp39-manylinux1_x86_64.whl (7.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /shared-libs/python3.9/py/lib/python3.9/site-packages (from torchtext==0.10.0) (2.28.1)\nCollecting torch==1.9.0\n  Downloading torch-1.9.0-cp39-cp39-manylinux1_x86_64.whl (831.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.4/831.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /shared-libs/python3.9/py/lib/python3.9/site-packages (from torchtext==0.10.0) (4.64.1)\nRequirement already satisfied: numpy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from torchtext==0.10.0) (1.23.4)\nRequirement already satisfied: typing-extensions in /shared-libs/python3.9/py/lib/python3.9/site-packages (from torch==1.9.0->torchtext==0.10.0) (4.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests->torchtext==0.10.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests->torchtext==0.10.0) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests->torchtext==0.10.0) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests->torchtext==0.10.0) (2.1.1)\nInstalling collected packages: torch, torchtext\n  Attempting uninstall: torch\n    Found existing installation: torch 1.12.1\n    Not uninstalling torch at /shared-libs/python3.9/py/lib/python3.9/site-packages, outside environment /root/venv\n    Can't uninstall 'torch'. No files were found to uninstall.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.13.1 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-1.9.0 torchtext-0.10.0\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\n\nSEED = 17\ntorch.manual_seed(SEED)","metadata":{"tags":[],"cell_id":"be288846e0984a7b93d63d3c53025d13","source_hash":"d8a7d644","execution_start":1685535251087,"execution_millis":372,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"},{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"<torch._C.Generator at 0x7f18b475cbf0>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"def set_device():\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  if device != \"cuda\":\n    print(\"WARNING: For this notebook to perform best, \"\n        \"if possible, in the menu under `Runtime` -> \"\n        \"`Change runtime type.`  select `GPU` \")\n  else:\n    print(\"GPU is enabled in this notebook.\")\n\n  return device\n\nDEVICE = set_device()","metadata":{"cell_id":"105171a957374da8855e14d5b6bff501","source_hash":"b1afd054","execution_start":1685535253594,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -> `Change runtime type.`  select `GPU` \n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Veri okuma kısmı (hazır)","metadata":{"tags":[],"cell_id":"e2742efda3b347c7824ba95ad3f429c1","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"}},{"cell_type":"code","source":"import torch\nimport torchtext.legacy.data as data\nimport torchtext.legacy.datasets as datasets\nimport random\nimport nltk\n\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('brown')\nnltk.download('webtext')\n\nSEED = 1234\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\n\ndef load_dataset(sentence_length=50, batch_size=32, seed=SEED):\n    TEXT = data.Field(sequential=True,\n                      tokenize=nltk.word_tokenize,\n                      lower=True,\n                      include_lengths=True,\n                      batch_first=True,\n                      fix_length=sentence_length)\n    LABEL = data.LabelField(dtype=torch.float)\n\n    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n    \n    # Veri kümesini dilimle\n    train_data, val_data = train_data.split(split_ratio=0.8, random_state=random.seed(seed))\n    test_data = test_data[:500]\n\n    # Vocab'ları oluşturun\n    TEXT.build_vocab(train_data, max_size=10000)\n    LABEL.build_vocab(train_data)\n    \n    # DataLoader'ları oluşturun\n    train_loader, val_loader, test_loader = data.BucketIterator.splits(\n        (train_data, val_data, test_data),\n        batch_size=batch_size,\n        sort_key=lambda x: len(x.text),\n        sort_within_batch=True\n    )\n\n    return train_loader, val_loader, test_loader, TEXT\n\ntrain_loader, val_loader, test_loader, TEXT = load_dataset(sentence_length=50, batch_size=32, seed=SEED)\n","metadata":{"cell_id":"9a85916cbabc4de9a69700cbefa1cd90","source_hash":"34a1438e","execution_start":1685535259246,"execution_millis":175716,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n[nltk_data] Downloading package brown to /root/nltk_data...\n[nltk_data]   Unzipping corpora/brown.zip.\n[nltk_data] Downloading package webtext to /root/nltk_data...\n[nltk_data]   Unzipping corpora/webtext.zip.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def text_from_dict(arr, dictionary):\n  text = []\n  for element in arr:\n    text.append(dictionary[element])\n  return text\n\n\ndef view_data(TEXT, train_iter):\n  for idx, batch in enumerate(train_iter):\n    text = batch.text[0]\n    target = batch.label\n\n    for itr in range(25, 30):\n      print('Review: ', ' '.join(text_from_dict(text[itr], TEXT.vocab.itos)))\n      print('Label: ', int(target[itr].item()), '\\n')\n\n    print('[0: Negative Review, 1: Positive Review]')\n    if idx==0:\n      break\n\n# view_data'yı çağır, training set'teki verilerin az bir kısmını görüntüle/print et\nview_data(TEXT, train_loader)","metadata":{"tags":[],"cell_id":"050d8bb5a526461694725ace5a294252","source_hash":"bcb36eb7","execution_start":1685535434961,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Review:  in all my 60 years of age , i have learned that when we watch a movie there is an <unk> ( whether we want it or not ) <unk> with an specific character. < br / > < br / > sometimes because the character <unk> certain gesture ,\nLabel:  1 \n\nReview:  at first glance , it would seem natural to compare where the sidewalk ends with laura . both have <unk> qualities , both were directed by otto preminger , and both star dana andrews and gene tierney . but that 's where most of the comparisons end . laura dealt\nLabel:  1 \n\nReview:  there 's more to offer in the opening of the odd couple than in the entirety of most films . felix <unk> ( the poor guy 's <unk> even <unk> him ) checks into a new york hotel . a cleaning lady says `` good night . '' `` goodbye\nLabel:  1 \n\nReview:  `` the invisible ray '' is part science fiction and part horror . it was also the third of seven <unk> features . in this entry the <unk> role goes to boris <unk> < br / > < br / > through specially designed <unk> equipment , dr. <unk> rukh\nLabel:  1 \n\nReview:  i bought unhinged because i got <unk> by the gory picture on the cover . if you want to see all the good parts of the movie just look on the back of the box . all the kills are shown and i can honestly tell you that they look\nLabel:  0 \n\n[0: Negative Review, 1: Positive Review]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Bizim kodlarımız (our NN code)","metadata":{"tags":[],"cell_id":"d9569c2751f940ed81f522402bac4642","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"13757868f5414aa3bdf27ddce2ce2db9","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# dropout ekleyebilir misiniz? her layer arasına, az miktar\nclass MLP(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden1, hidden2, hidden3, num_classes):\n        super(MLP, self).__init__()\n\n        self.drop = nn.Dropout(p=0.1)\n        self.embedding = nn.Embedding(vocab_size, embedding_dim) # bu kısım kelimeleri özniteliklere çevirmek için\n        self.fc1 = nn.Linear(embedding_dim, hidden1)\n        self.fc2 = nn.Linear(hidden1, hidden2)\n        self.fc3 = nn.Linear(hidden2, hidden3)\n        self.fc4 = nn.Linear(hidden3, num_classes)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, text, text_lengths):\n        # alt 3 satırı chatgpt'den aldım\n        embedded = self.embedding(text)\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n        packed_output, _ = nn.utils.rnn.pad_packed_sequence(packed_embedded, batch_first=True)\n        \n        x = self.drop(self.relu(self.fc1(packed_output[:, -1, :])))\n        x = self.drop(self.relu(self.fc2(x)))\n        x = self.drop(self.relu(self.fc3(x)))\n        output = self.sigmoid(self.fc4(x))\n        return output\n\n\"\"\"\nVeriye uygun bir aktivasyon fonksiyonu olarak, \ngenellikle çok sınıflı sınıflandırma problemlerinde tercih edilen \n\"Softmax\" fonksiyonunu kullanabilirsiniz\n    -ChatGPT\n\"\"\"\n","metadata":{"tags":[],"cell_id":"5c43146c3cbe41c4bf24385296705141","source_hash":"5131fbaf","execution_start":1685535434964,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"'\\nVeriye uygun bir aktivasyon fonksiyonu olarak, \\ngenellikle çok sınıflı sınıflandırma problemlerinde tercih edilen \\n\"Softmax\" fonksiyonunu kullanabilirsiniz\\n    -ChatGPT\\n'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# training, earlystop'lu validation kodu\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# train_loader, val_loader, test_loader\n\nvocab_size = len(TEXT.vocab)\nembedding_dim = 100\nhidden1 = 64\nhidden2 = 16\nhidden3 = 8\n\npatience = 2\n\nmodel = MLP(vocab_size, embedding_dim, hidden1, hidden2, hidden3, 1)\n\n# Loss fonksiyonu ve optimizer tanımlama\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001) # learning rate seçin\n\nbatch_size = 32\nnum_epochs = 100\n\n# Eğitim döngüsü\nbest_val_loss = 10000\nearly_stopping_counter = 0\n\ntrain_losses, val_losses = [], []\nfor epoch in range(num_epochs):\n    model.train()\n\n    avg_train_loss = 0\n    for batch in train_loader:\n        \n        text, text_lengths = batch.text\n        # Gradientleri sıfırlama\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(text, text_lengths)\n        \n        # Loss hesaplama\n        loss = criterion(outputs.squeeze(), batch.label)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Parametre güncelleme\n        optimizer.step()\n\n        avg_train_loss += loss\n\n    curr_train_loss = avg_train_loss / len(train_loader)\n    train_losses.append(curr_train_loss)\n\n    # Doğrulama (validation) döngüsü\n    model.eval()\n    avg_val_loss = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            \n            text, text_lengths = batch.text\n            # Forward pass\n            val_outputs = model(text, text_lengths)\n            \n            # Loss hesaplama\n            val_loss = criterion(val_outputs.squeeze(), batch.label)\n\n            avg_val_loss += loss\n\n    curr_val_loss = avg_val_loss / len(val_loader)\n    val_losses.append(curr_val_loss)\n\n    # En iyi doğruluk skorunu kontrol et - Accuracy değil loss'a göre değiştir\n    if curr_val_loss < best_val_loss:\n        best_val_loss = curr_val_loss\n        early_stopping_counter = 0\n    else:\n        early_stopping_counter += 1\n        print(\"Earlystop counter \", early_stopping_counter)\n        \n    # Early stopping kontrolü\n    if early_stopping_counter >= patience:\n            print(\"Early stopping!\")\n            break\n        \n    # Ekrana yazdırma\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {curr_train_loss:.4f}, Val Loss: {curr_val_loss:.4f}\")\n\n\n","metadata":{"cell_id":"3fd1ad4e29894a62beb44261cae2556d","source_hash":"d8b57ff4","execution_start":1685536530028,"execution_millis":79713,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epoch [1/100], Train Loss: 0.6934, Val Loss: 0.6927\nEpoch [2/100], Train Loss: 0.6924, Val Loss: 0.6925\nEpoch [3/100], Train Loss: 0.6895, Val Loss: 0.6613\nEarlystop counter  1\nEpoch [4/100], Train Loss: 0.6818, Val Loss: 0.6997\nEarlystop counter  2\nEarly stopping!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# test setinden sonuç alan kodu - bunu da siz yazarsınız artık :D\n\n","metadata":{"cell_id":"48e489b8bdd14033a1aac3b93c1b7c58","source_hash":"1e466dbd","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=700e962d-3fcf-4c82-8c8d-f1f2cc26fdf7' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"6db66e1c3fda4e20b8612ab4230d072c","deepnote_execution_queue":[]}}